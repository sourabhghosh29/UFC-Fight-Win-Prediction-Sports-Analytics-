{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fedata.csv\")\n",
    "df=df.drop(['winner'], axis=1)\n",
    "df1 = pd.read_csv(\"data_full.csv\")\n",
    "features = df.columns\n",
    "# import labelencoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# instantiate labelencoder object\n",
    "le = LabelEncoder()\n",
    "df_y = pd.DataFrame(df1['winner'])\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = df_y.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = df_y.columns[categorical_feature_mask].tolist()\n",
    "# apply le on categorical feature columns\n",
    "df1[categorical_cols] = df1[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "X_all = df[features]\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_all =df1['winner']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(887, input_dim=41, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B__Prev</th>\n",
       "      <th>B__Age</th>\n",
       "      <th>B__Height</th>\n",
       "      <th>B__Weight</th>\n",
       "      <th>Last_round</th>\n",
       "      <th>Max_round</th>\n",
       "      <th>R__Prev</th>\n",
       "      <th>R__Age</th>\n",
       "      <th>R__Height</th>\n",
       "      <th>R__Weight</th>\n",
       "      <th>...</th>\n",
       "      <th>StrikesDistance Body Punches</th>\n",
       "      <th>StrikesDistance Body Kicks</th>\n",
       "      <th>StrikesPunches</th>\n",
       "      <th>StrikesBody Significant Strikes</th>\n",
       "      <th>StrikesHead Total Strikes</th>\n",
       "      <th>StrikesSignificant Strikes</th>\n",
       "      <th>B_HomeTown</th>\n",
       "      <th>B_Location</th>\n",
       "      <th>R_HomeTown</th>\n",
       "      <th>R_Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>329</td>\n",
       "      <td>558</td>\n",
       "      <td>594</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>34.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>4.830508</td>\n",
       "      <td>517</td>\n",
       "      <td>22</td>\n",
       "      <td>640</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.436849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.680702</td>\n",
       "      <td>531</td>\n",
       "      <td>419</td>\n",
       "      <td>338</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.081802</td>\n",
       "      <td>511</td>\n",
       "      <td>409</td>\n",
       "      <td>513</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.258737</td>\n",
       "      <td>6.160916</td>\n",
       "      <td>73.895385</td>\n",
       "      <td>92</td>\n",
       "      <td>526</td>\n",
       "      <td>234</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>38.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>140</td>\n",
       "      <td>138</td>\n",
       "      <td>262</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>6</td>\n",
       "      <td>34.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.786441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.027366</td>\n",
       "      <td>79</td>\n",
       "      <td>62</td>\n",
       "      <td>582</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>3</td>\n",
       "      <td>39.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.775510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.905562</td>\n",
       "      <td>556</td>\n",
       "      <td>421</td>\n",
       "      <td>536</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>193</td>\n",
       "      <td>79</td>\n",
       "      <td>18</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>2</td>\n",
       "      <td>37.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.188054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.836364</td>\n",
       "      <td>222</td>\n",
       "      <td>532</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1854 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      B__Prev  B__Age  B__Height  B__Weight  Last_round  Max_round  R__Prev  \\\n",
       "254         0    31.0      172.0       61.0           1          3        1   \n",
       "1032        1    40.0      180.0       77.0           1          3        4   \n",
       "2206        2    25.0      172.0       65.0           3          3       10   \n",
       "937         4    30.0      180.0       77.0           3          3        4   \n",
       "648         1    34.0      180.0       70.0           3          3        3   \n",
       "...       ...     ...        ...        ...         ...        ...      ...   \n",
       "1033        1    37.0      190.0      106.0           3          3        4   \n",
       "1731        6    34.0      167.0       61.0           1          3        3   \n",
       "763         3    39.0      175.0       70.0           3          3        3   \n",
       "835         0    32.0      172.0       70.0           3          3        0   \n",
       "1653        2    37.0      190.0       84.0           3          3        2   \n",
       "\n",
       "      R__Age  R__Height  R__Weight  ...  StrikesDistance Body Punches  \\\n",
       "254     36.0      180.0       61.0  ...                           0.0   \n",
       "1032    34.0      185.0       84.0  ...                           0.0   \n",
       "2206    30.0      177.0       65.0  ...                           0.0   \n",
       "937     40.0      185.0       77.0  ...                           0.0   \n",
       "648     28.0      180.0       70.0  ...                           0.0   \n",
       "...      ...        ...        ...  ...                           ...   \n",
       "1033    38.0      195.0      118.0  ...                           0.0   \n",
       "1731    37.0      172.0       61.0  ...                           0.0   \n",
       "763     35.0      172.0       70.0  ...                           0.0   \n",
       "835     29.0      172.0       70.0  ...                           0.0   \n",
       "1653     0.0        0.0        0.0  ...                           0.0   \n",
       "\n",
       "      StrikesDistance Body Kicks  StrikesPunches  \\\n",
       "254                          0.0             0.0   \n",
       "1032                         0.0             0.0   \n",
       "2206                         0.0             0.0   \n",
       "937                          0.0             0.0   \n",
       "648                          0.0             0.0   \n",
       "...                          ...             ...   \n",
       "1033                         0.0             0.0   \n",
       "1731                         0.0             0.0   \n",
       "763                          0.0             0.0   \n",
       "835                          0.0             0.0   \n",
       "1653                         0.0             0.0   \n",
       "\n",
       "      StrikesBody Significant Strikes  StrikesHead Total Strikes  \\\n",
       "254                          0.000000                   0.000000   \n",
       "1032                         0.000000                   0.269231   \n",
       "2206                         3.436849                   0.000000   \n",
       "937                          0.000000                   0.000000   \n",
       "648                         12.258737                   6.160916   \n",
       "...                               ...                        ...   \n",
       "1033                         0.363248                   0.000000   \n",
       "1731                         1.786441                   0.000000   \n",
       "763                          4.775510                   0.000000   \n",
       "835                          0.000000                   0.000000   \n",
       "1653                         6.188054                   0.000000   \n",
       "\n",
       "      StrikesSignificant Strikes  B_HomeTown  B_Location  R_HomeTown  \\\n",
       "254                     0.000000         329         558         594   \n",
       "1032                    4.830508         517          22         640   \n",
       "2206                   62.680702         531         419         338   \n",
       "937                    18.081802         511         409         513   \n",
       "648                    73.895385          92         526         234   \n",
       "...                          ...         ...         ...         ...   \n",
       "1033                    0.000000         140         138         262   \n",
       "1731                    7.027366          79          62         582   \n",
       "763                     9.905562         556         421         536   \n",
       "835                     0.000000         193          79          18   \n",
       "1653                   62.836364         222         532          -1   \n",
       "\n",
       "      R_Location  \n",
       "254          456  \n",
       "1032         491  \n",
       "2206         291  \n",
       "937          397  \n",
       "648           38  \n",
       "...          ...  \n",
       "1033           4  \n",
       "1731         448  \n",
       "763          414  \n",
       "835          121  \n",
       "1653          -1  \n",
       "\n",
       "[1854 rows x 41 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 3 3 3 3 0\n",
      "\t [[{{node loss_10/dense_22_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 3 0 0 3 0\n",
      "\t [[{{node loss_11/dense_24_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 0 0 3 3 3\n",
      "\t [[{{node loss_12/dense_26_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 3 0 0 3 3\n",
      "\t [[{{node loss_13/dense_28_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 3 0 0 0 0\n",
      "\t [[{{node loss_14/dense_30_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 0 0 0 3 0\n",
      "\t [[{{node loss_15/dense_32_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 0 3 3 3 3\n",
      "\t [[{{node loss_16/dense_34_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 0 0 0 3 3\n",
      "\t [[{{node loss_17/dense_36_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 3 0 0 3 3\n",
      "\t [[{{node loss_18/dense_38_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: nan% (nan%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 3 which is outside the valid range of [0, 1).  Label values: 3 0 3 3 3\n",
      "\t [[{{node loss_19/dense_40_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(168, input_dim=168,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#log_dir = \"logs/fit/\" +\"events.out.tfevents.1587536516.LAPTOP-QOGBL7D6\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1854/1854 [==============================] - 1s 574us/step - loss: -4942.7753 - accuracy: 0.0086\n",
      "Epoch 2/100\n",
      "1854/1854 [==============================] - 1s 424us/step - loss: -1514986.5093 - accuracy: 0.0070\n",
      "Epoch 3/100\n",
      "1854/1854 [==============================] - 1s 501us/step - loss: -35318807.4789 - accuracy: 0.0070\n",
      "Epoch 4/100\n",
      "1854/1854 [==============================] - 1s 530us/step - loss: -278103718.9606 - accuracy: 0.0070\n",
      "Epoch 5/100\n",
      "1854/1854 [==============================] - 1s 530us/step - loss: -1290506202.3258 - accuracy: 0.0070\n",
      "Epoch 6/100\n",
      "1854/1854 [==============================] - 1s 526us/step - loss: -4183867748.9385 - accuracy: 0.0070\n",
      "Epoch 7/100\n",
      "1854/1854 [==============================] - 1s 541us/step - loss: -11191077941.2816 - accuracy: 0.0070\n",
      "Epoch 8/100\n",
      "1854/1854 [==============================] - 1s 558us/step - loss: -25668834252.9105 - accuracy: 0.0070\n",
      "Epoch 9/100\n",
      "1854/1854 [==============================] - 1s 536us/step - loss: -53179634580.4358 - accuracy: 0.0070\n",
      "Epoch 10/100\n",
      "1854/1854 [==============================] - 1s 557us/step - loss: -101579845335.5426 - accuracy: 0.0070\n",
      "Epoch 11/100\n",
      "1854/1854 [==============================] - 1s 586us/step - loss: -177056906469.7648 - accuracy: 0.0070\n",
      "Epoch 12/100\n",
      "1854/1854 [==============================] - 1s 581us/step - loss: -289421086985.1133 - accuracy: 0.0070\n",
      "Epoch 13/100\n",
      "1854/1854 [==============================] - 1s 544us/step - loss: -450056574583.3010 - accuracy: 0.0070\n",
      "Epoch 14/100\n",
      "1854/1854 [==============================] - 1s 623us/step - loss: -672054596425.4584 - accuracy: 0.0070\n",
      "Epoch 15/100\n",
      "1854/1854 [==============================] - 1s 642us/step - loss: -968485618842.6494 - accuracy: 0.0070\n",
      "Epoch 16/100\n",
      "1854/1854 [==============================] - 1s 560us/step - loss: -1353925679123.8835 - accuracy: 0.0070\n",
      "Epoch 17/100\n",
      "1854/1854 [==============================] - 1s 613us/step - loss: -1849985002198.8523 - accuracy: 0.0070\n",
      "Epoch 18/100\n",
      "1854/1854 [==============================] - 1s 604us/step - loss: -2471699991608.3364 - accuracy: 0.0070\n",
      "Epoch 19/100\n",
      "1854/1854 [==============================] - 1s 599us/step - loss: -3242289317862.5933 - accuracy: 0.0070\n",
      "Epoch 20/100\n",
      "1854/1854 [==============================] - 1s 590us/step - loss: -4183424144469.0571 - accuracy: 0.0070\n",
      "Epoch 21/100\n",
      "1854/1854 [==============================] - 1s 592us/step - loss: -5319380881518.4639 - accuracy: 0.0070\n",
      "Epoch 22/100\n",
      "1854/1854 [==============================] - 1s 585us/step - loss: -6674005767058.6406 - accuracy: 0.0070\n",
      "Epoch 23/100\n",
      "1854/1854 [==============================] - 1s 591us/step - loss: -8271040668328.4570 - accuracy: 0.0070\n",
      "Epoch 24/100\n",
      "1854/1854 [==============================] - 1s 587us/step - loss: -10159614218155.4941 - accuracy: 0.0070\n",
      "Epoch 25/100\n",
      "1854/1854 [==============================] - 1s 583us/step - loss: -12355176872304.9492 - accuracy: 0.0070\n",
      "Epoch 26/100\n",
      "1854/1854 [==============================] - 1s 588us/step - loss: -14884312929709.7051 - accuracy: 0.0070\n",
      "Epoch 27/100\n",
      "1854/1854 [==============================] - 1s 590us/step - loss: -17808863380039.5273 - accuracy: 0.0070\n",
      "Epoch 28/100\n",
      "1854/1854 [==============================] - 1s 606us/step - loss: -21151732925653.1953 - accuracy: 0.0070\n",
      "Epoch 29/100\n",
      "1854/1854 [==============================] - 1s 602us/step - loss: -24975384752689.1602 - accuracy: 0.0070\n",
      "Epoch 30/100\n",
      "1854/1854 [==============================] - 1s 602us/step - loss: -29296060735885.6719 - accuracy: 0.0070\n",
      "Epoch 31/100\n",
      "1854/1854 [==============================] - 1s 614us/step - loss: -34176191430656.0000 - accuracy: 0.0070\n",
      "Epoch 32/100\n",
      "1854/1854 [==============================] - 1s 592us/step - loss: -39603979845808.7422 - accuracy: 0.0070\n",
      "Epoch 33/100\n",
      "1854/1854 [==============================] - 1s 597us/step - loss: -45666162664723.0547 - accuracy: 0.0070\n",
      "Epoch 34/100\n",
      "1854/1854 [==============================] - 1s 625us/step - loss: -52371253406802.8516 - accuracy: 0.0070\n",
      "Epoch 35/100\n",
      "1854/1854 [==============================] - 1s 584us/step - loss: -59755253527089.1562 - accuracy: 0.0070\n",
      "Epoch 36/100\n",
      "1854/1854 [==============================] - 1s 539us/step - loss: -67945607164005.6250 - accuracy: 0.0070\n",
      "Epoch 37/100\n",
      "1854/1854 [==============================] - 1s 510us/step - loss: -76958882675976.0156 - accuracy: 0.0070\n",
      "Epoch 38/100\n",
      "1854/1854 [==============================] - 1s 540us/step - loss: -86896018382485.6875 - accuracy: 0.0070\n",
      "Epoch 39/100\n",
      "1854/1854 [==============================] - 1s 554us/step - loss: -97760845917991.4844 - accuracy: 0.0070\n",
      "Epoch 40/100\n",
      "1854/1854 [==============================] - 1s 525us/step - loss: -109650674361362.7656 - accuracy: 0.0070\n",
      "Epoch 41/100\n",
      "1854/1854 [==============================] - 1s 545us/step - loss: -122711296553998.3594 - accuracy: 0.0070\n",
      "Epoch 42/100\n",
      "1854/1854 [==============================] - 1s 465us/step - loss: -136953159535212.8125 - accuracy: 0.0070\n",
      "Epoch 43/100\n",
      "1854/1854 [==============================] - 1s 597us/step - loss: -152463682912909.9375 - accuracy: 0.0070\n",
      "Epoch 44/100\n",
      "1854/1854 [==============================] - 1s 604us/step - loss: -169306730571468.9062 - accuracy: 0.0070\n",
      "Epoch 45/100\n",
      "1854/1854 [==============================] - 1s 613us/step - loss: -187543296794756.5312 - accuracy: 0.0070\n",
      "Epoch 46/100\n",
      "1854/1854 [==============================] - 1s 600us/step - loss: -207402375741899.5312 - accuracy: 0.0070\n",
      "Epoch 47/100\n",
      "1854/1854 [==============================] - 1s 618us/step - loss: -228830739537186.5000 - accuracy: 0.0070\n",
      "Epoch 48/100\n",
      "1854/1854 [==============================] - 1s 601us/step - loss: -252012810940181.8125 - accuracy: 0.0070\n",
      "Epoch 49/100\n",
      "1854/1854 [==============================] - 1s 454us/step - loss: -276807981314591.4688 - accuracy: 0.0070\n",
      "Epoch 50/100\n",
      "1854/1854 [==============================] - 1s 589us/step - loss: -303220117009934.9375 - accuracy: 0.0070\n",
      "Epoch 51/100\n",
      "1854/1854 [==============================] - 1s 576us/step - loss: -331825919475058.0625 - accuracy: 0.0070\n",
      "Epoch 52/100\n",
      "1854/1854 [==============================] - 1s 391us/step - loss: -362533181697884.5000 - accuracy: 0.0070\n",
      "Epoch 53/100\n",
      "1854/1854 [==============================] - 1s 529us/step - loss: -395252747659398.7500 - accuracy: 0.0070\n",
      "Epoch 54/100\n",
      "1854/1854 [==============================] - 1s 545us/step - loss: -430307213496660.2500 - accuracy: 0.0070\n",
      "Epoch 55/100\n",
      "1854/1854 [==============================] - 1s 577us/step - loss: -467884276661370.6250 - accuracy: 0.0070\n",
      "Epoch 56/100\n",
      "1854/1854 [==============================] - 1s 540us/step - loss: -507784533653873.0000 - accuracy: 0.0070\n",
      "Epoch 57/100\n",
      "1854/1854 [==============================] - 1s 518us/step - loss: -550422840499543.5625 - accuracy: 0.0070\n",
      "Epoch 58/100\n",
      "1854/1854 [==============================] - 1s 528us/step - loss: -596073073389475.2500 - accuracy: 0.0070\n",
      "Epoch 59/100\n",
      "1854/1854 [==============================] - 1s 521us/step - loss: -644887075126483.0000 - accuracy: 0.0070\n",
      "Epoch 60/100\n",
      "1854/1854 [==============================] - 1s 552us/step - loss: -696612184903808.1250 - accuracy: 0.0070\n",
      "Epoch 61/100\n",
      "1854/1854 [==============================] - 1s 531us/step - loss: -751138697329786.6250 - accuracy: 0.0070\n",
      "Epoch 62/100\n",
      "1854/1854 [==============================] - 1s 541us/step - loss: -808612852162303.7500 - accuracy: 0.0070\n",
      "Epoch 63/100\n",
      "1854/1854 [==============================] - 1s 536us/step - loss: -869954706672042.3750 - accuracy: 0.0070\n",
      "Epoch 64/100\n",
      "1854/1854 [==============================] - 1s 541us/step - loss: -934463543847276.6250 - accuracy: 0.0070\n",
      "Epoch 65/100\n",
      "1854/1854 [==============================] - 1s 519us/step - loss: -1003032218611195.0000 - accuracy: 0.0070\n",
      "Epoch 66/100\n",
      "1854/1854 [==============================] - 1s 515us/step - loss: -1075597767911399.7500 - accuracy: 0.0070\n",
      "Epoch 67/100\n",
      "1854/1854 [==============================] - 1s 556us/step - loss: -1152349519617067.2500 - accuracy: 0.0070\n",
      "Epoch 68/100\n",
      "1854/1854 [==============================] - 1s 514us/step - loss: -1233721194723697.0000 - accuracy: 0.0070\n",
      "Epoch 69/100\n",
      "1854/1854 [==============================] - 1s 517us/step - loss: -1319901346044158.0000 - accuracy: 0.0070\n",
      "Epoch 70/100\n",
      "1854/1854 [==============================] - 1s 593us/step - loss: -1410472933827381.7500 - accuracy: 0.0070\n",
      "Epoch 71/100\n",
      "1854/1854 [==============================] - 1s 543us/step - loss: -1505672200701300.2500 - accuracy: 0.0070\n",
      "Epoch 72/100\n",
      "1854/1854 [==============================] - 1s 521us/step - loss: -1606018964749028.2500 - accuracy: 0.0070\n",
      "Epoch 73/100\n",
      "1854/1854 [==============================] - 1s 534us/step - loss: -1711805864571264.5000 - accuracy: 0.0070\n",
      "Epoch 74/100\n",
      "1854/1854 [==============================] - 1s 471us/step - loss: -1822796460119063.2500 - accuracy: 0.0070\n",
      "Epoch 75/100\n",
      "1854/1854 [==============================] - 1s 471us/step - loss: -1939143134561122.0000 - accuracy: 0.0070\n",
      "Epoch 76/100\n",
      "1854/1854 [==============================] - 1s 529us/step - loss: -2061523310925247.2500 - accuracy: 0.0070\n",
      "Epoch 77/100\n",
      "1854/1854 [==============================] - 1s 494us/step - loss: -2188995283626737.5000 - accuracy: 0.0070\n",
      "Epoch 78/100\n",
      "1854/1854 [==============================] - 1s 503us/step - loss: -2323095058597483.5000 - accuracy: 0.0070\n",
      "Epoch 79/100\n",
      "1854/1854 [==============================] - 1s 518us/step - loss: -2463695331858438.5000 - accuracy: 0.0070\n",
      "Epoch 80/100\n",
      "1854/1854 [==============================] - 1s 494us/step - loss: -2611548662825843.5000 - accuracy: 0.0070\n",
      "Epoch 81/100\n",
      "1854/1854 [==============================] - 1s 532us/step - loss: -2765978890733570.0000 - accuracy: 0.0070\n",
      "Epoch 82/100\n",
      "1854/1854 [==============================] - 1s 507us/step - loss: -2927110283006593.0000 - accuracy: 0.0070\n",
      "Epoch 83/100\n",
      "1854/1854 [==============================] - 1s 525us/step - loss: -3095066621483050.0000 - accuracy: 0.0070\n",
      "Epoch 84/100\n",
      "1854/1854 [==============================] - 1s 499us/step - loss: -3268842012223923.5000 - accuracy: 0.0070\n",
      "Epoch 85/100\n",
      "1854/1854 [==============================] - 1s 518us/step - loss: -3450257276853910.0000 - accuracy: 0.0070\n",
      "Epoch 86/100\n",
      "1854/1854 [==============================] - 1s 488us/step - loss: -3640886157650426.0000 - accuracy: 0.0070\n",
      "Epoch 87/100\n",
      "1854/1854 [==============================] - 1s 493us/step - loss: -3838239624908079.5000 - accuracy: 0.0070\n",
      "Epoch 88/100\n",
      "1854/1854 [==============================] - 1s 507us/step - loss: -4044858095946066.0000 - accuracy: 0.0070\n",
      "Epoch 89/100\n",
      "1854/1854 [==============================] - 1s 507us/step - loss: -4261665676510085.5000 - accuracy: 0.0070\n",
      "Epoch 90/100\n",
      "1854/1854 [==============================] - 1s 508us/step - loss: -4489820058924824.0000 - accuracy: 0.0070\n",
      "Epoch 91/100\n",
      "1854/1854 [==============================] - 1s 497us/step - loss: -4727410278505904.0000 - accuracy: 0.0070\n",
      "Epoch 92/100\n",
      "1854/1854 [==============================] - 1s 494us/step - loss: -4974067743021787.0000 - accuracy: 0.0070\n",
      "Epoch 93/100\n",
      "1854/1854 [==============================] - 1s 498us/step - loss: -5229783972309197.0000 - accuracy: 0.0070\n",
      "Epoch 94/100\n",
      "1854/1854 [==============================] - 1s 493us/step - loss: -5498886970563362.0000 - accuracy: 0.0070\n",
      "Epoch 95/100\n",
      "1854/1854 [==============================] - 1s 493us/step - loss: -5778136193370680.0000 - accuracy: 0.0070\n",
      "Epoch 96/100\n",
      "1854/1854 [==============================] - 1s 498us/step - loss: -6069612893789740.0000 - accuracy: 0.0070\n",
      "Epoch 97/100\n",
      "1854/1854 [==============================] - 1s 502us/step - loss: -6373241848177320.0000 - accuracy: 0.0070\n",
      "Epoch 98/100\n",
      "1854/1854 [==============================] - 1s 499us/step - loss: -6689015825275063.0000 - accuracy: 0.0070\n",
      "Epoch 99/100\n",
      "1854/1854 [==============================] - 1s 494us/step - loss: -7018881069366465.0000 - accuracy: 0.0070\n",
      "Epoch 100/100\n",
      "1854/1854 [==============================] - 1s 520us/step - loss: -7360200320790152.0000 - accuracy: 0.0070\n",
      "1854/1854 [==============================] - 0s 134us/step\n",
      "Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "### new method\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=41, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, Y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GraphViz\n",
      "  Downloading graphviz-0.13.2-py2.py3-none-any.whl (17 kB)\n",
      "Installing collected packages: GraphViz\n",
      "Successfully installed GraphViz-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GraphViz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "#plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1854/1854 [==============================] - 1s 322us/step - loss: -7713042793835356.0000 - accuracy: 0.0070\n",
      "Epoch 2/100\n",
      "1854/1854 [==============================] - 1s 519us/step - loss: -8078257171511417.0000 - accuracy: 0.0070\n",
      "Epoch 3/100\n",
      "1854/1854 [==============================] - 1s 593us/step - loss: -8458180052828637.0000 - accuracy: 0.0070\n",
      "Epoch 4/100\n",
      "1854/1854 [==============================] - 1s 610us/step - loss: -8852724667222619.0000 - accuracy: 0.0070\n",
      "Epoch 5/100\n",
      "1854/1854 [==============================] - 1s 625us/step - loss: -9259776672318058.0000 - accuracy: 0.0070\n",
      "Epoch 6/100\n",
      "1854/1854 [==============================] - 1s 604us/step - loss: -9684451789658490.0000 - accuracy: 0.0070\n",
      "Epoch 7/100\n",
      "1854/1854 [==============================] - 1s 572us/step - loss: -10122504227199264.0000 - accuracy: 0.0070\n",
      "Epoch 8/100\n",
      "1854/1854 [==============================] - 1s 551us/step - loss: -10576611784204226.0000 - accuracy: 0.0070\n",
      "Epoch 9/100\n",
      "1854/1854 [==============================] - 1s 611us/step - loss: -11046695092303888.0000 - accuracy: 0.0070\n",
      "Epoch 10/100\n",
      "1854/1854 [==============================] - 1s 608us/step - loss: -11533184835450596.0000 - accuracy: 0.0070\n",
      "Epoch 11/100\n",
      "1854/1854 [==============================] - 1s 644us/step - loss: -12037021418195794.0000 - accuracy: 0.0070\n",
      "Epoch 12/100\n",
      "1854/1854 [==============================] - 1s 608us/step - loss: -12551712492241196.0000 - accuracy: 0.0070\n",
      "Epoch 13/100\n",
      "1854/1854 [==============================] - 1s 590us/step - loss: -13090232658516274.0000 - accuracy: 0.0070\n",
      "Epoch 14/100\n",
      "1854/1854 [==============================] - 1s 596us/step - loss: -13648289154019552.0000 - accuracy: 0.0070\n",
      "Epoch 15/100\n",
      "1854/1854 [==============================] - 1s 593us/step - loss: -14224566391114188.0000 - accuracy: 0.0070\n",
      "Epoch 16/100\n",
      "1854/1854 [==============================] - 1s 588us/step - loss: -14816697489973698.0000 - accuracy: 0.0070\n",
      "Epoch 17/100\n",
      "1854/1854 [==============================] - 1s 597us/step - loss: -15432646075256306.0000 - accuracy: 0.0070\n",
      "Epoch 18/100\n",
      "1854/1854 [==============================] - 1s 594us/step - loss: -16068764357497402.0000 - accuracy: 0.0070\n",
      "Epoch 19/100\n",
      "1854/1854 [==============================] - 1s 615us/step - loss: -16725107157607646.0000 - accuracy: 0.0070\n",
      "Epoch 20/100\n",
      "1854/1854 [==============================] - 1s 601us/step - loss: -17405448841626484.0000 - accuracy: 0.0070\n",
      "Epoch 21/100\n",
      "1854/1854 [==============================] - 1s 606us/step - loss: -18103186184132596.0000 - accuracy: 0.0070\n",
      "Epoch 22/100\n",
      "1854/1854 [==============================] - 1s 593us/step - loss: -18824782579021664.0000 - accuracy: 0.0070\n",
      "Epoch 23/100\n",
      "1854/1854 [==============================] - 1s 590us/step - loss: -19566455230390936.0000 - accuracy: 0.0070\n",
      "Epoch 24/100\n",
      "1854/1854 [==============================] - 1s 591us/step - loss: -20333733343819176.0000 - accuracy: 0.0070\n",
      "Epoch 25/100\n",
      "1854/1854 [==============================] - 1s 587us/step - loss: -21122613450545968.0000 - accuracy: 0.0070\n",
      "Epoch 26/100\n",
      "1854/1854 [==============================] - 1s 597us/step - loss: -21933731058935024.0000 - accuracy: 0.0070\n",
      "Epoch 27/100\n",
      "1854/1854 [==============================] - 1s 640us/step - loss: -22777204356345640.0000 - accuracy: 0.0070\n",
      "Epoch 28/100\n",
      "1854/1854 [==============================] - 1s 618us/step - loss: -23646415195416444.0000 - accuracy: 0.0070\n",
      "Epoch 29/100\n",
      "1854/1854 [==============================] - 1s 584us/step - loss: -24544957465538892.0000 - accuracy: 0.0070\n",
      "Epoch 30/100\n",
      "1854/1854 [==============================] - 1s 559us/step - loss: -25469582822979776.0000 - accuracy: 0.0070\n",
      "Epoch 31/100\n",
      "1854/1854 [==============================] - 1s 494us/step - loss: -26421284493495512.0000 - accuracy: 0.0070\n",
      "Epoch 32/100\n",
      "1854/1854 [==============================] - 1s 500us/step - loss: -27399238232390352.0000 - accuracy: 0.0070\n",
      "Epoch 33/100\n",
      "1854/1854 [==============================] - 1s 520us/step - loss: -28414892374424396.0000 - accuracy: 0.0070\n",
      "Epoch 34/100\n",
      "1854/1854 [==============================] - 1s 519us/step - loss: -29455476328988072.0000 - accuracy: 0.0070\n",
      "Epoch 35/100\n",
      "1854/1854 [==============================] - 1s 554us/step - loss: -30531643052357100.0000 - accuracy: 0.0070\n",
      "Epoch 36/100\n",
      "1854/1854 [==============================] - 1s 584us/step - loss: -31632867742205620.0000 - accuracy: 0.0070\n",
      "Epoch 37/100\n",
      "1854/1854 [==============================] - 1s 590us/step - loss: -32775399367022424.0000 - accuracy: 0.0070\n",
      "Epoch 38/100\n",
      "1854/1854 [==============================] - 1s 572us/step - loss: -33938963146399100.0000 - accuracy: 0.0070\n",
      "Epoch 39/100\n",
      "1854/1854 [==============================] - 1s 609us/step - loss: -35143434978338896.0000 - accuracy: 0.0070\n",
      "Epoch 40/100\n",
      "1854/1854 [==============================] - 1s 572us/step - loss: -36393783864776408.0000 - accuracy: 0.0070\n",
      "Epoch 41/100\n",
      "1854/1854 [==============================] - 1s 559us/step - loss: -37686814383149904.0000 - accuracy: 0.0070\n",
      "Epoch 42/100\n",
      "1854/1854 [==============================] - 1s 458us/step - loss: -39005753889010952.0000 - accuracy: 0.0070\n",
      "Epoch 43/100\n",
      "1854/1854 [==============================] - 1s 616us/step - loss: -40376152026277560.0000 - accuracy: 0.0070\n",
      "Epoch 44/100\n",
      "1854/1854 [==============================] - 1s 599us/step - loss: -41782588112231168.0000 - accuracy: 0.0070\n",
      "Epoch 45/100\n",
      "1854/1854 [==============================] - 1s 595us/step - loss: -43229235428053544.0000 - accuracy: 0.0070\n",
      "Epoch 46/100\n",
      "1854/1854 [==============================] - 1s 594us/step - loss: -44706941233261320.0000 - accuracy: 0.0070\n",
      "Epoch 47/100\n",
      "1854/1854 [==============================] - 1s 596us/step - loss: -46221787051862808.0000 - accuracy: 0.0070\n",
      "Epoch 48/100\n",
      "1854/1854 [==============================] - 1s 590us/step - loss: -47790652035815336.0000 - accuracy: 0.0070\n",
      "Epoch 49/100\n",
      "1854/1854 [==============================] - 1s 611us/step - loss: -49391574257464240.0000 - accuracy: 0.0070\n",
      "Epoch 50/100\n",
      "1854/1854 [==============================] - 1s 603us/step - loss: -51052701553957688.0000 - accuracy: 0.0070\n",
      "Epoch 51/100\n",
      "1854/1854 [==============================] - 1s 602us/step - loss: -52738844680294296.0000 - accuracy: 0.0070\n",
      "Epoch 52/100\n",
      "1854/1854 [==============================] - 1s 572us/step - loss: -54475036208938176.0000 - accuracy: 0.0070\n",
      "Epoch 53/100\n",
      "1854/1854 [==============================] - 1s 364us/step - loss: -56251511634368840.0000 - accuracy: 0.0070\n",
      "Epoch 54/100\n",
      "1854/1854 [==============================] - 1s 593us/step - loss: -58064481131192912.0000 - accuracy: 0.0070\n",
      "Epoch 55/100\n",
      "1854/1854 [==============================] - 1s 573us/step - loss: -59927680012406240.0000 - accuracy: 0.0070\n",
      "Epoch 56/100\n",
      "1854/1854 [==============================] - 1s 563us/step - loss: -61833583108416752.0000 - accuracy: 0.0070\n",
      "Epoch 57/100\n",
      "1854/1854 [==============================] - 1s 522us/step - loss: -63783240932911456.0000 - accuracy: 0.0070\n",
      "Epoch 58/100\n",
      "1854/1854 [==============================] - 1s 404us/step - loss: -65800244970110680.0000 - accuracy: 0.0070\n",
      "Epoch 59/100\n",
      "1854/1854 [==============================] - 1s 447us/step - loss: -67862174104649832.0000 - accuracy: 0.0070\n",
      "Epoch 60/100\n",
      "1854/1854 [==============================] - 1s 344us/step - loss: -69984283693351408.0000 - accuracy: 0.0070\n",
      "Epoch 61/100\n",
      "1854/1854 [==============================] - 1s 438us/step - loss: -72158197555639520.0000 - accuracy: 0.0070\n",
      "Epoch 62/100\n",
      "1854/1854 [==============================] - 1s 600us/step - loss: -74348848018296624.0000 - accuracy: 0.0070\n",
      "Epoch 63/100\n",
      "1854/1854 [==============================] - 1s 599us/step - loss: -76604950154322496.0000 - accuracy: 0.0070\n",
      "Epoch 64/100\n",
      "1854/1854 [==============================] - 1s 640us/step - loss: -78928227624918032.0000 - accuracy: 0.0070\n",
      "Epoch 65/100\n",
      "1854/1854 [==============================] - 1s 587us/step - loss: -81294875446215216.0000 - accuracy: 0.0070\n",
      "Epoch 66/100\n",
      "1854/1854 [==============================] - 1s 515us/step - loss: -83728811505434000.0000 - accuracy: 0.0070\n",
      "Epoch 67/100\n",
      "1854/1854 [==============================] - 1s 538us/step - loss: -86207278812625680.0000 - accuracy: 0.0070\n",
      "Epoch 68/100\n",
      "1854/1854 [==============================] - 1s 546us/step - loss: -88744451362073152.0000 - accuracy: 0.0070\n",
      "Epoch 69/100\n",
      "1854/1854 [==============================] - 1s 527us/step - loss: -91350243894406784.0000 - accuracy: 0.0070\n",
      "Epoch 70/100\n",
      "1854/1854 [==============================] - 1s 557us/step - loss: -94006842714741360.0000 - accuracy: 0.0070\n",
      "Epoch 71/100\n",
      "1854/1854 [==============================] - 1s 532us/step - loss: -96739363325093408.0000 - accuracy: 0.0070\n",
      "Epoch 72/100\n",
      "1854/1854 [==============================] - 1s 532us/step - loss: -99547351307273904.0000 - accuracy: 0.0070\n",
      "Epoch 73/100\n",
      "1854/1854 [==============================] - 1s 534us/step - loss: -102443168339429120.0000 - accuracy: 0.0070\n",
      "Epoch 74/100\n",
      "1854/1854 [==============================] - 1s 526us/step - loss: -105405578377130944.0000 - accuracy: 0.0070\n",
      "Epoch 75/100\n",
      "1854/1854 [==============================] - 1s 526us/step - loss: -108440565926299264.0000 - accuracy: 0.0070\n",
      "Epoch 76/100\n",
      "1854/1854 [==============================] - 1s 574us/step - loss: -111531642170444624.0000 - accuracy: 0.0070\n",
      "Epoch 77/100\n",
      "1854/1854 [==============================] - 1s 572us/step - loss: -114694679886448448.0000 - accuracy: 0.0070\n",
      "Epoch 78/100\n",
      "1854/1854 [==============================] - 1s 572us/step - loss: -117906065030580640.0000 - accuracy: 0.0070\n",
      "Epoch 79/100\n",
      "1854/1854 [==============================] - 1s 580us/step - loss: -121201116745672752.0000 - accuracy: 0.0070\n",
      "Epoch 80/100\n",
      "1854/1854 [==============================] - 1s 601us/step - loss: -124568005150350912.0000 - accuracy: 0.0070\n",
      "Epoch 81/100\n",
      "1854/1854 [==============================] - 1s 574us/step - loss: -128027242718013904.0000 - accuracy: 0.0070\n",
      "Epoch 82/100\n",
      "1854/1854 [==============================] - 1s 579us/step - loss: -131551795535520992.0000 - accuracy: 0.0070\n",
      "Epoch 83/100\n",
      "1854/1854 [==============================] - 1s 574us/step - loss: -135146431158715536.0000 - accuracy: 0.0070\n",
      "Epoch 84/100\n",
      "1854/1854 [==============================] - 1s 577us/step - loss: -138813650311564864.0000 - accuracy: 0.0070\n",
      "Epoch 85/100\n",
      "1854/1854 [==============================] - 1s 568us/step - loss: -142566320773097776.0000 - accuracy: 0.0070\n",
      "Epoch 86/100\n",
      "1854/1854 [==============================] - 1s 573us/step - loss: -146373537485758976.0000 - accuracy: 0.0070\n",
      "Epoch 87/100\n",
      "1854/1854 [==============================] - 1s 577us/step - loss: -150289916606322528.0000 - accuracy: 0.0070\n",
      "Epoch 88/100\n",
      "1854/1854 [==============================] - 1s 593us/step - loss: -154300016305114720.0000 - accuracy: 0.0070\n",
      "Epoch 89/100\n",
      "1854/1854 [==============================] - 1s 581us/step - loss: -158365703128537504.0000 - accuracy: 0.0070\n",
      "Epoch 90/100\n",
      "1854/1854 [==============================] - 1s 578us/step - loss: -162522244130057888.0000 - accuracy: 0.0070\n",
      "Epoch 91/100\n",
      "1854/1854 [==============================] - 1s 577us/step - loss: -166795534635170496.0000 - accuracy: 0.0070\n",
      "Epoch 92/100\n",
      "1854/1854 [==============================] - 1s 576us/step - loss: -171164558995761376.0000 - accuracy: 0.0070\n",
      "Epoch 93/100\n",
      "1854/1854 [==============================] - 1s 586us/step - loss: -175598235300030848.0000 - accuracy: 0.0070\n",
      "Epoch 94/100\n",
      "1854/1854 [==============================] - 1s 595us/step - loss: -180149811398361696.0000 - accuracy: 0.0070\n",
      "Epoch 95/100\n",
      "1854/1854 [==============================] - 1s 626us/step - loss: -184808319258585728.0000 - accuracy: 0.0070\n",
      "Epoch 96/100\n",
      "1854/1854 [==============================] - 1s 578us/step - loss: -189557920938968576.0000 - accuracy: 0.0070\n",
      "Epoch 97/100\n",
      "1854/1854 [==============================] - 1s 555us/step - loss: -194373973155060128.0000 - accuracy: 0.0070\n",
      "Epoch 98/100\n",
      "1854/1854 [==============================] - 1s 606us/step - loss: -199306953761402368.0000 - accuracy: 0.0070\n",
      "Epoch 99/100\n",
      "1854/1854 [==============================] - 1s 533us/step - loss: -204333627327273184.0000 - accuracy: 0.0070\n",
      "Epoch 100/100\n",
      "1854/1854 [==============================] - 1s 535us/step - loss: -209495424759666080.0000 - accuracy: 0.0070\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 168)               28392     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 168)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1690      \n",
      "=================================================================\n",
      "Total params: 30,082\n",
      "Trainable params: 30,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", \n",
    "                               monitor = 'val_acc',\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.2Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\soura\\\\anaconda3\\\\envs\\\\tf-gpu\\\\lib\\\\site-packages\\\\~cipy\\\\fft\\\\_pocketfft\\\\pypocketfft.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading scipy-1.2.0-cp37-cp37m-win_amd64.whl (31.7 MB)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from scipy==1.2) (1.18.1)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n"
     ]
    }
   ],
   "source": [
    "pip install scipy==1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting talosNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading talos-0.6.6-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from talos) (1.18.1)\n",
      "Collecting kerasplotlib\n",
      "  Downloading kerasplotlib-0.1.6.tar.gz (3.5 kB)\n",
      "Collecting keras==2.3.0\n",
      "  Downloading Keras-2.3.0-py2.py3-none-any.whl (377 kB)\n",
      "Collecting astetik\n",
      "  Downloading astetik-1.9.9.tar.gz (32 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from talos) (1.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from talos) (2.23.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from talos) (0.0)\n",
      "Collecting chances\n",
      "  Downloading chances-0.1.9.tar.gz (35 kB)\n",
      "Collecting wrangle\n",
      "  Downloading wrangle-0.6.7.tar.gz (25 kB)\n",
      "Collecting tensorflow==1.14.0\n",
      "  Downloading tensorflow-1.14.0-cp37-cp37m-win_amd64.whl (68.3 MB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
      "Requirement already satisfied: statsmodels>=0.11.0 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from talos) (0.11.1)\n",
      "Requirement already satisfied: ipython in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from kerasplotlib->talos) (7.13.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from keras==2.3.0->talos) (1.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from keras==2.3.0->talos) (1.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from keras==2.3.0->talos) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from keras==2.3.0->talos) (5.3.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from keras==2.3.0->talos) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from keras==2.3.0->talos) (1.4.1)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.10.0-py3-none-any.whl (215 kB)\n",
      "Collecting geonamescache\n",
      "  Downloading geonamescache-1.1.0-py3-none-any.whl (830 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from pandas->talos) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from pandas->talos) (2.8.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests->talos) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests->talos) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests->talos) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests->talos) (1.25.8)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from sklearn->talos) (0.22.2.post1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==1.14.0->talos) (1.1.0)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
      "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==1.14.0->talos) (1.27.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==1.14.0->talos) (0.34.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==1.14.0->talos) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==1.14.0->talos) (0.8.0)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0\n",
      "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==1.14.0->talos) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==1.14.0->talos) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==1.14.0->talos) (3.11.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorflow==1.14.0->talos) (1.12.1)\n",
      "Requirement already satisfied: patsy>=0.5 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from statsmodels>=0.11.0->talos) (0.5.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ipython->kerasplotlib->talos) (0.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ipython->kerasplotlib->talos) (4.3.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ipython->kerasplotlib->talos) (46.1.3.post20200330)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ipython->kerasplotlib->talos) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ipython->kerasplotlib->talos) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ipython->kerasplotlib->talos) (0.16.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ipython->kerasplotlib->talos) (2.6.1)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ipython->kerasplotlib->talos) (0.4.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ipython->kerasplotlib->talos) (3.0.4)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from seaborn->astetik->talos) (3.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn->sklearn->talos) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->talos) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->talos) (0.14.1)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from traitlets>=4.2->ipython->kerasplotlib->talos) (0.2.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from jedi>=0.10->ipython->kerasplotlib->talos) (0.6.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kerasplotlib->talos) (0.1.9)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.1.2->seaborn->astetik->talos) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.1.2->seaborn->astetik->talos) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\soura\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.1.2->seaborn->astetik->talos) (1.2.0)\n",
      "Building wheels for collected packages: kerasplotlib, astetik, chances, wrangle\n",
      "  Building wheel for kerasplotlib (setup.py): started\n",
      "  Building wheel for kerasplotlib (setup.py): finished with status 'done'\n",
      "  Created wheel for kerasplotlib: filename=kerasplotlib-0.1.6-py3-none-any.whl size=3608 sha256=fc1715e7270e566c7eb1f4e590a3d9f8d2b60fb5b336ceeba448d28ec16100bf\n",
      "  Stored in directory: c:\\users\\soura\\appdata\\local\\pip\\cache\\wheels\\1c\\b4\\c8\\d1533d85f7fc617e3201c3f41b79fe49ae9284c8fc4a5bd4b2\n",
      "  Building wheel for astetik (setup.py): started\n",
      "  Building wheel for astetik (setup.py): finished with status 'done'\n",
      "  Created wheel for astetik: filename=astetik-1.9.9-py3-none-any.whl size=56971 sha256=9d9da097bd5eafa330a7d9086f9f061a1770e3d4347dce1e1a8388e9aa79b7dc\n",
      "  Stored in directory: c:\\users\\soura\\appdata\\local\\pip\\cache\\wheels\\b2\\6f\\46\\33d4db84472eb1f4b6a9bfdc12fab667ee37e1cd10eff9d584\n",
      "  Building wheel for chances (setup.py): started\n",
      "  Building wheel for chances (setup.py): finished with status 'done'\n",
      "  Created wheel for chances: filename=chances-0.1.9-py3-none-any.whl size=41613 sha256=e7d11ecab751159f7b228f3f88f4a71d5246fd0d656394b57e170a352c75b104\n",
      "  Stored in directory: c:\\users\\soura\\appdata\\local\\pip\\cache\\wheels\\f3\\2e\\7e\\316f7da11ccf2195ff05e4a0186a4b5975be9bd0b0004198b6\n",
      "  Building wheel for wrangle (setup.py): started\n",
      "  Building wheel for wrangle (setup.py): finished with status 'done'\n",
      "  Created wheel for wrangle: filename=wrangle-0.6.7-py3-none-any.whl size=49899 sha256=304dd02c353fca17ed73e219dc9f669cea675b85d71cc0c45d3c91d8e337596f\n",
      "  Stored in directory: c:\\users\\soura\\appdata\\local\\pip\\cache\\wheels\\ed\\30\\6f\\0ef272d1dc45d60081669b1bb8b6122dd5107cd5afb29b2598\n",
      "Successfully built kerasplotlib astetik chances wrangle\n",
      "Installing collected packages: kerasplotlib, keras, seaborn, wrangle, geonamescache, astetik, chances, tensorflow-estimator, tensorboard, tensorflow, tqdm, talos\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.3.1\n",
      "    Uninstalling Keras-2.3.1:\n",
      "      Successfully uninstalled Keras-2.3.1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.1.0\n",
      "    Uninstalling tensorboard-2.1.0:\n",
      "      Successfully uninstalled tensorboard-2.1.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.1.0\n",
      "    Uninstalling tensorflow-2.1.0:\n",
      "      Successfully uninstalled tensorflow-2.1.0\n",
      "Successfully installed astetik-1.9.9 chances-0.1.9 geonamescache-1.1.0 keras-2.3.0 kerasplotlib-0.1.6 seaborn-0.10.0 talos-0.6.6 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 tqdm-4.45.0 wrangle-0.6.7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: wrangle 0.6.7 has requirement scipy==1.2, but you'll have scipy 1.4.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can go ahead and set the parameter space\n",
    "p = {'lr': (0.5, 5, 10),\n",
    "     'first_neuron':[4, 8, 16, 32, 64],\n",
    "     'hidden_layers':[0, 1, 2],\n",
    "     'batch_size': (2, 30, 10),\n",
    "     'epochs': [150],\n",
    "     'dropout': (0, 0.5, 5),\n",
    "     'weight_regulizer':[None],\n",
    "     'emb_output_dims': [None],\n",
    "     'shape':['brick','long_funnel'],\n",
    "     'optimizer': ['adam', 'nadam', 'RMSprop'],\n",
    "     'losses': ['logcosh', 'binary_crossentropy'],\n",
    "     'activation':['relu', 'elu'],\n",
    "     'last_activation': ['sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 18s - loss: 663.4927 - acc: 0.0000e+ - ETA: 1s - loss: 361.2121 - acc: 0.2812     - ETA: 0s - loss: 265.2721 - acc: 0.384 - ETA: 0s - loss: 220.9129 - acc: 0.420 - ETA: 0s - loss: 192.4419 - acc: 0.438 - ETA: 0s - loss: 172.7057 - acc: 0.451 - 1s 678us/sample - loss: 166.3853 - acc: 0.4536 - val_loss: 61.0971 - val_acc: 0.5625\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 89.2423 - acc: 0.59 - ETA: 0s - loss: 109.1893 - acc: 0.562 - ETA: 0s - loss: 97.1045 - acc: 0.526 - ETA: 0s - loss: 99.4663 - acc: 0.53 - ETA: 0s - loss: 94.2778 - acc: 0.52 - ETA: 0s - loss: 90.3833 - acc: 0.51 - ETA: 0s - loss: 86.1238 - acc: 0.51 - 0s 211us/sample - loss: 85.4687 - acc: 0.5167 - val_loss: 54.5984 - val_acc: 0.5517\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 64.2175 - acc: 0.62 - ETA: 0s - loss: 74.3486 - acc: 0.51 - ETA: 0s - loss: 66.3710 - acc: 0.54 - ETA: 0s - loss: 67.9233 - acc: 0.52 - ETA: 0s - loss: 71.0421 - acc: 0.53 - ETA: 0s - loss: 67.6718 - acc: 0.51 - ETA: 0s - loss: 67.3800 - acc: 0.52 - 0s 214us/sample - loss: 66.1608 - acc: 0.5259 - val_loss: 47.4778 - val_acc: 0.4310\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 81.8554 - acc: 0.34 - ETA: 0s - loss: 51.6361 - acc: 0.44 - ETA: 0s - loss: 47.2673 - acc: 0.51 - ETA: 0s - loss: 44.0240 - acc: 0.51 - ETA: 0s - loss: 45.5553 - acc: 0.51 - ETA: 0s - loss: 44.6093 - acc: 0.51 - ETA: 0s - loss: 48.0310 - acc: 0.49 - ETA: 0s - loss: 46.1361 - acc: 0.49 - ETA: 0s - loss: 43.1164 - acc: 0.50 - ETA: 0s - loss: 42.2432 - acc: 0.50 - 1s 313us/sample - loss: 41.9791 - acc: 0.5059 - val_loss: 19.2286 - val_acc: 0.5151\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 31.8730 - acc: 0.43 - ETA: 0s - loss: 33.0092 - acc: 0.52 - ETA: 0s - loss: 33.1679 - acc: 0.54 - ETA: 0s - loss: 32.3975 - acc: 0.55 - ETA: 0s - loss: 33.0945 - acc: 0.54 - ETA: 0s - loss: 33.7930 - acc: 0.53 - ETA: 0s - loss: 31.3705 - acc: 0.53 - ETA: 0s - loss: 30.6121 - acc: 0.53 - ETA: 0s - loss: 29.9197 - acc: 0.53 - 0s 269us/sample - loss: 29.4240 - acc: 0.5318 - val_loss: 17.1361 - val_acc: 0.4763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fe16c31148>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(histogram_freq=1)\n",
    "\n",
    "model.fit(x=X_train, \n",
    "          y=encoded_Y, \n",
    "          epochs=5, \n",
    "          validation_data=(X_test, encoded_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(input_dim=41,units=hp.Int('units',\n",
    "                                        min_value=300,\n",
    "                                        max_value=552,\n",
    "                                        step=32),\n",
    "                           activation='relu'))\n",
    "    model.add(layers.Dense(171, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ss\\ss\\oracle.json\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_acc',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "directory='ss',\n",
    "    project_name='ss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 6s - loss: 147.3794 - acc: 0.0000e+0 - ETA: 0s - loss: 39.2705 - acc: 0.4928    - ETA: 0s - loss: 30.4367 - acc: 0.46 - ETA: 0s - loss: 25.6600 - acc: 0.48 - ETA: 0s - loss: 21.0454 - acc: 0.47 - ETA: 0s - loss: 18.8056 - acc: 0.48 - 1s 481us/sample - loss: 18.0584 - acc: 0.4860 - val_loss: 9.7913 - val_acc: 0.5819\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 10.1412 - acc: 0.50 - ETA: 0s - loss: 5.8985 - acc: 0.5365 - ETA: 0s - loss: 5.3058 - acc: 0.519 - ETA: 0s - loss: 4.3207 - acc: 0.511 - ETA: 0s - loss: 3.9221 - acc: 0.512 - ETA: 0s - loss: 3.5118 - acc: 0.515 - 0s 188us/sample - loss: 3.3826 - acc: 0.5129 - val_loss: 1.3755 - val_acc: 0.4763\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.1291 - acc: 0.562 - ETA: 0s - loss: 1.9247 - acc: 0.541 - ETA: 0s - loss: 2.2375 - acc: 0.523 - ETA: 0s - loss: 2.3638 - acc: 0.523 - ETA: 0s - loss: 2.1120 - acc: 0.531 - ETA: 0s - loss: 2.0107 - acc: 0.535 - ETA: 0s - loss: 2.0401 - acc: 0.529 - 0s 216us/sample - loss: 1.9107 - acc: 0.5329 - val_loss: 1.4438 - val_acc: 0.5259\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.2777 - acc: 0.406 - ETA: 0s - loss: 1.0401 - acc: 0.527 - ETA: 0s - loss: 0.9783 - acc: 0.531 - ETA: 0s - loss: 1.0016 - acc: 0.539 - ETA: 0s - loss: 1.0058 - acc: 0.548 - ETA: 0s - loss: 1.0970 - acc: 0.534 - ETA: 0s - loss: 1.1296 - acc: 0.533 - ETA: 0s - loss: 1.1135 - acc: 0.539 - 0s 246us/sample - loss: 1.1221 - acc: 0.5410 - val_loss: 1.0604 - val_acc: 0.5259\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.7317 - acc: 0.562 - ETA: 0s - loss: 1.0360 - acc: 0.587 - ETA: 0s - loss: 0.9722 - acc: 0.574 - ETA: 0s - loss: 0.9649 - acc: 0.580 - ETA: 0s - loss: 0.9951 - acc: 0.556 - ETA: 0s - loss: 1.0233 - acc: 0.545 - ETA: 0s - loss: 1.1219 - acc: 0.530 - 0s 208us/sample - loss: 1.1158 - acc: 0.5329 - val_loss: 1.1781 - val_acc: 0.4957\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 6s - loss: 97.3111 - acc: 0.0000e+ - ETA: 0s - loss: 47.5770 - acc: 0.4832   - ETA: 0s - loss: 34.5837 - acc: 0.48 - ETA: 0s - loss: 26.1164 - acc: 0.49 - ETA: 0s - loss: 21.1780 - acc: 0.50 - ETA: 0s - loss: 17.0021 - acc: 0.51 - 0s 252us/sample - loss: 16.5104 - acc: 0.5081 - val_loss: 3.7420 - val_acc: 0.5797\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 5.2493 - acc: 0.500 - ETA: 0s - loss: 4.3177 - acc: 0.502 - ETA: 0s - loss: 3.7110 - acc: 0.521 - ETA: 0s - loss: 3.2346 - acc: 0.514 - ETA: 0s - loss: 3.0605 - acc: 0.515 - ETA: 0s - loss: 2.7243 - acc: 0.517 - 0s 173us/sample - loss: 2.6760 - acc: 0.5135 - val_loss: 1.2934 - val_acc: 0.5474\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.8732 - acc: 0.625 - ETA: 0s - loss: 1.1568 - acc: 0.531 - ETA: 0s - loss: 1.3191 - acc: 0.519 - ETA: 0s - loss: 1.2306 - acc: 0.520 - ETA: 0s - loss: 1.2258 - acc: 0.526 - ETA: 0s - loss: 1.2831 - acc: 0.533 - 0s 173us/sample - loss: 1.2924 - acc: 0.5307 - val_loss: 1.1444 - val_acc: 0.5259\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.5895 - acc: 0.687 - ETA: 0s - loss: 1.1783 - acc: 0.520 - ETA: 0s - loss: 1.1004 - acc: 0.517 - ETA: 0s - loss: 1.1142 - acc: 0.510 - ETA: 0s - loss: 1.1136 - acc: 0.501 - 1s 317us/sample - loss: 1.1190 - acc: 0.5022 - val_loss: 1.1736 - val_acc: 0.5841\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.2267 - acc: 0.500 - ETA: 0s - loss: 1.1458 - acc: 0.517 - ETA: 0s - loss: 1.2743 - acc: 0.512 - ETA: 0s - loss: 1.2063 - acc: 0.505 - ETA: 0s - loss: 1.5093 - acc: 0.507 - ETA: 0s - loss: 1.3961 - acc: 0.510 - 0s 190us/sample - loss: 1.3415 - acc: 0.5124 - val_loss: 1.1112 - val_acc: 0.5797\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 6s - loss: 129.6207 - acc: 0.0000e+0 - ETA: 0s - loss: 59.3448 - acc: 0.4609    - ETA: 0s - loss: 41.0527 - acc: 0.47 - ETA: 0s - loss: 31.3377 - acc: 0.49 - ETA: 0s - loss: 24.5601 - acc: 0.48 - 0s 239us/sample - loss: 20.1196 - acc: 0.5000 - val_loss: 1.9136 - val_acc: 0.4978\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 2.6585 - acc: 0.593 - ETA: 0s - loss: 2.0139 - acc: 0.517 - ETA: 0s - loss: 1.7645 - acc: 0.523 - ETA: 0s - loss: 1.6604 - acc: 0.520 - ETA: 0s - loss: 1.6024 - acc: 0.527 - 0s 148us/sample - loss: 1.5733 - acc: 0.5291 - val_loss: 1.2094 - val_acc: 0.4526\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.5597 - acc: 0.406 - ETA: 0s - loss: 1.3789 - acc: 0.516 - ETA: 0s - loss: 1.2951 - acc: 0.516 - ETA: 0s - loss: 1.2482 - acc: 0.525 - ETA: 0s - loss: 1.2477 - acc: 0.526 - 0s 157us/sample - loss: 1.2472 - acc: 0.5183 - val_loss: 1.1717 - val_acc: 0.5603\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.8131 - acc: 0.625 - ETA: 0s - loss: 0.8425 - acc: 0.576 - ETA: 0s - loss: 0.8833 - acc: 0.552 - ETA: 0s - loss: 0.8765 - acc: 0.555 - ETA: 0s - loss: 0.8997 - acc: 0.552 - 0s 167us/sample - loss: 0.8817 - acc: 0.5604 - val_loss: 0.9985 - val_acc: 0.5647\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.7811 - acc: 0.468 - ETA: 0s - loss: 0.7606 - acc: 0.570 - ETA: 0s - loss: 0.7946 - acc: 0.550 - ETA: 0s - loss: 0.7720 - acc: 0.558 - ETA: 0s - loss: 0.7916 - acc: 0.546 - ETA: 0s - loss: 0.7976 - acc: 0.547 - 0s 173us/sample - loss: 0.7936 - acc: 0.5512 - val_loss: 0.8570 - val_acc: 0.5776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 204bd612ef35da28a1313470dace6f5b</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5811781883239746</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 396</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 6s - loss: 92.1803 - acc: 0.0000e+ - ETA: 0s - loss: 24.6315 - acc: 0.3516   - ETA: 0s - loss: 17.3393 - acc: 0.42 - ETA: 0s - loss: 13.8951 - acc: 0.44 - ETA: 0s - loss: 12.3100 - acc: 0.44 - ETA: 0s - loss: 11.4746 - acc: 0.44 - ETA: 0s - loss: 10.6287 - acc: 0.45 - 1s 538us/sample - loss: 10.2850 - acc: 0.4579 - val_loss: 3.9605 - val_acc: 0.5797\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 3.4465 - acc: 0.593 - ETA: 0s - loss: 3.8210 - acc: 0.542 - ETA: 0s - loss: 3.1039 - acc: 0.529 - ETA: 0s - loss: 2.7524 - acc: 0.539 - ETA: 0s - loss: 2.6786 - acc: 0.550 - ETA: 0s - loss: 2.5874 - acc: 0.544 - 0s 198us/sample - loss: 2.5171 - acc: 0.5458 - val_loss: 2.4243 - val_acc: 0.5539\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.9232 - acc: 0.531 - ETA: 0s - loss: 1.9495 - acc: 0.545 - ETA: 0s - loss: 2.4108 - acc: 0.537 - ETA: 0s - loss: 2.4888 - acc: 0.518 - ETA: 0s - loss: 2.5967 - acc: 0.515 - ETA: 0s - loss: 2.4421 - acc: 0.530 - ETA: 0s - loss: 2.2736 - acc: 0.539 - 0s 217us/sample - loss: 2.2468 - acc: 0.5367 - val_loss: 1.8838 - val_acc: 0.5647\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.1090 - acc: 0.625 - ETA: 0s - loss: 1.2605 - acc: 0.564 - ETA: 0s - loss: 1.2513 - acc: 0.567 - ETA: 0s - loss: 1.2253 - acc: 0.574 - ETA: 0s - loss: 1.2794 - acc: 0.575 - ETA: 0s - loss: 1.4849 - acc: 0.567 - ETA: 0s - loss: 1.5166 - acc: 0.569 - ETA: 0s - loss: 1.6449 - acc: 0.565 - 0s 263us/sample - loss: 1.6912 - acc: 0.5609 - val_loss: 1.9743 - val_acc: 0.4547\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.2636 - acc: 0.531 - ETA: 0s - loss: 1.5370 - acc: 0.562 - ETA: 0s - loss: 1.4072 - acc: 0.572 - ETA: 0s - loss: 1.4250 - acc: 0.559 - ETA: 0s - loss: 1.4594 - acc: 0.558 - ETA: 0s - loss: 1.4521 - acc: 0.566 - ETA: 0s - loss: 1.5075 - acc: 0.567 - ETA: 0s - loss: 1.4897 - acc: 0.564 - ETA: 0s - loss: 1.5289 - acc: 0.561 - ETA: 0s - loss: 1.5279 - acc: 0.559 - 1s 303us/sample - loss: 1.5217 - acc: 0.5615 - val_loss: 2.0996 - val_acc: 0.4828\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 6s - loss: 143.9261 - acc: 0.0000e+0 - ETA: 0s - loss: 44.0338 - acc: 0.3750    - ETA: 0s - loss: 27.8277 - acc: 0.44 - ETA: 0s - loss: 21.0590 - acc: 0.47 - ETA: 0s - loss: 17.3094 - acc: 0.46 - ETA: 0s - loss: 15.0140 - acc: 0.46 - 0s 265us/sample - loss: 13.9692 - acc: 0.4703 - val_loss: 2.7872 - val_acc: 0.4914\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 5.3318 - acc: 0.312 - ETA: 0s - loss: 2.9870 - acc: 0.500 - ETA: 0s - loss: 2.7055 - acc: 0.511 - ETA: 0s - loss: 2.4968 - acc: 0.513 - ETA: 0s - loss: 2.4218 - acc: 0.512 - ETA: 0s - loss: 2.3545 - acc: 0.514 - 0s 204us/sample - loss: 2.3615 - acc: 0.5140 - val_loss: 2.1557 - val_acc: 0.4591\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.5020 - acc: 0.500 - ETA: 0s - loss: 2.6180 - acc: 0.541 - ETA: 0s - loss: 3.0935 - acc: 0.555 - ETA: 0s - loss: 3.0774 - acc: 0.550 - ETA: 0s - loss: 2.9135 - acc: 0.553 - ETA: 0s - loss: 2.8231 - acc: 0.547 - ETA: 0s - loss: 2.7527 - acc: 0.542 - ETA: 0s - loss: 2.7561 - acc: 0.536 - 0s 258us/sample - loss: 2.7239 - acc: 0.5356 - val_loss: 2.7490 - val_acc: 0.5560\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 2.0805 - acc: 0.562 - ETA: 0s - loss: 1.5742 - acc: 0.531 - ETA: 0s - loss: 1.4946 - acc: 0.533 - ETA: 0s - loss: 1.5360 - acc: 0.531 - ETA: 0s - loss: 1.5970 - acc: 0.539 - ETA: 0s - loss: 1.6679 - acc: 0.546 - ETA: 0s - loss: 1.7441 - acc: 0.547 - ETA: 0s - loss: 1.9568 - acc: 0.537 - 0s 255us/sample - loss: 2.0258 - acc: 0.5372 - val_loss: 3.0925 - val_acc: 0.4461\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 2.1601 - acc: 0.468 - ETA: 0s - loss: 1.8975 - acc: 0.541 - ETA: 0s - loss: 2.2962 - acc: 0.555 - ETA: 0s - loss: 2.4226 - acc: 0.548 - ETA: 0s - loss: 2.2917 - acc: 0.551 - ETA: 0s - loss: 2.3052 - acc: 0.551 - ETA: 0s - loss: 2.2060 - acc: 0.539 - ETA: 0s - loss: 2.1469 - acc: 0.543 - 0s 251us/sample - loss: 2.1584 - acc: 0.5388 - val_loss: 2.6811 - val_acc: 0.5603\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 6s - loss: 115.4674 - acc: 0.0000e+0 - ETA: 0s - loss: 28.5563 - acc: 0.3938    - ETA: 0s - loss: 19.2224 - acc: 0.46 - ETA: 0s - loss: 15.5912 - acc: 0.47 - ETA: 0s - loss: 12.9661 - acc: 0.48 - 0s 227us/sample - loss: 12.6127 - acc: 0.4876 - val_loss: 6.6014 - val_acc: 0.5517\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 4.8223 - acc: 0.656 - ETA: 0s - loss: 4.6230 - acc: 0.512 - ETA: 0s - loss: 3.6496 - acc: 0.529 - ETA: 0s - loss: 3.3366 - acc: 0.522 - ETA: 0s - loss: 3.1830 - acc: 0.521 - 0s 147us/sample - loss: 3.1780 - acc: 0.5178 - val_loss: 3.1405 - val_acc: 0.4138\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 2.2284 - acc: 0.375 - ETA: 0s - loss: 2.4337 - acc: 0.516 - ETA: 0s - loss: 2.2748 - acc: 0.528 - ETA: 0s - loss: 2.2187 - acc: 0.529 - ETA: 0s - loss: 2.1224 - acc: 0.544 - 0s 145us/sample - loss: 2.1291 - acc: 0.5464 - val_loss: 2.6103 - val_acc: 0.4353\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 3.2651 - acc: 0.531 - ETA: 0s - loss: 2.3364 - acc: 0.537 - ETA: 0s - loss: 2.4261 - acc: 0.553 - ETA: 0s - loss: 2.4044 - acc: 0.554 - ETA: 0s - loss: 2.6265 - acc: 0.549 - ETA: 0s - loss: 2.8462 - acc: 0.538 - 0s 180us/sample - loss: 2.9318 - acc: 0.5351 - val_loss: 5.2504 - val_acc: 0.4116\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 5.3552 - acc: 0.343 - ETA: 0s - loss: 3.7903 - acc: 0.534 - ETA: 0s - loss: 2.8437 - acc: 0.557 - ETA: 0s - loss: 2.5070 - acc: 0.573 - ETA: 0s - loss: 2.4364 - acc: 0.551 - 0s 161us/sample - loss: 2.3277 - acc: 0.5448 - val_loss: 1.9563 - val_acc: 0.4828\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 850ce44fc214420f5a5e740bc1754b1c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5639367699623108</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 396</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 5s - loss: 116.9538 - acc: 0.0000e+0 - ETA: 0s - loss: 44.4347 - acc: 0.4417    - ETA: 0s - loss: 31.7636 - acc: 0.47 - ETA: 0s - loss: 24.2685 - acc: 0.48 - ETA: 0s - loss: 20.5988 - acc: 0.48 - 1s 483us/sample - loss: 19.5093 - acc: 0.4865 - val_loss: 5.5683 - val_acc: 0.5862\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 3.8754 - acc: 0.593 - ETA: 0s - loss: 5.5455 - acc: 0.520 - ETA: 0s - loss: 4.2018 - acc: 0.496 - ETA: 0s - loss: 3.3795 - acc: 0.520 - ETA: 0s - loss: 2.9312 - acc: 0.509 - ETA: 0s - loss: 2.6110 - acc: 0.510 - ETA: 0s - loss: 2.4197 - acc: 0.515 - 0s 214us/sample - loss: 2.2827 - acc: 0.5167 - val_loss: 1.0671 - val_acc: 0.4763\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.9052 - acc: 0.531 - ETA: 0s - loss: 1.1776 - acc: 0.519 - ETA: 0s - loss: 1.0771 - acc: 0.524 - ETA: 0s - loss: 1.1304 - acc: 0.508 - ETA: 0s - loss: 1.0657 - acc: 0.514 - ETA: 0s - loss: 1.0389 - acc: 0.516 - 0s 182us/sample - loss: 1.0254 - acc: 0.5200 - val_loss: 1.2635 - val_acc: 0.5237\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.9916 - acc: 0.593 - ETA: 0s - loss: 1.2904 - acc: 0.539 - ETA: 0s - loss: 1.1724 - acc: 0.534 - ETA: 0s - loss: 1.1163 - acc: 0.527 - ETA: 0s - loss: 1.0880 - acc: 0.522 - ETA: 0s - loss: 1.0633 - acc: 0.523 - 0s 174us/sample - loss: 1.0592 - acc: 0.5237 - val_loss: 0.8955 - val_acc: 0.4677\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.2437 - acc: 0.406 - ETA: 0s - loss: 0.9319 - acc: 0.543 - ETA: 0s - loss: 0.8621 - acc: 0.533 - ETA: 0s - loss: 0.8344 - acc: 0.571 - ETA: 0s - loss: 0.8744 - acc: 0.558 - ETA: 0s - loss: 0.8920 - acc: 0.552 - ETA: 0s - loss: 0.8969 - acc: 0.546 - 0s 223us/sample - loss: 0.9300 - acc: 0.5318 - val_loss: 0.9441 - val_acc: 0.5345\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 6s - loss: 94.3291 - acc: 0.0000e+ - ETA: 0s - loss: 44.3029 - acc: 0.4609   - ETA: 0s - loss: 37.2852 - acc: 0.46 - ETA: 0s - loss: 30.8533 - acc: 0.47 - ETA: 0s - loss: 25.9020 - acc: 0.48 - ETA: 0s - loss: 22.5957 - acc: 0.48 - 1s 286us/sample - loss: 20.5529 - acc: 0.4865 - val_loss: 2.8549 - val_acc: 0.5409\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 6.3375 - acc: 0.343 - ETA: 0s - loss: 6.7571 - acc: 0.460 - ETA: 0s - loss: 5.6933 - acc: 0.500 - ETA: 0s - loss: 5.4094 - acc: 0.519 - ETA: 0s - loss: 4.9195 - acc: 0.504 - ETA: 0s - loss: 3.9126 - acc: 0.499 - ETA: 0s - loss: 3.4652 - acc: 0.505 - ETA: 0s - loss: 3.1563 - acc: 0.494 - 0s 241us/sample - loss: 3.0723 - acc: 0.4946 - val_loss: 1.8456 - val_acc: 0.4526\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.6716 - acc: 0.375 - ETA: 0s - loss: 1.5920 - acc: 0.556 - ETA: 0s - loss: 1.5361 - acc: 0.520 - ETA: 0s - loss: 1.4432 - acc: 0.524 - ETA: 0s - loss: 1.3537 - acc: 0.515 - ETA: 0s - loss: 1.3046 - acc: 0.522 - ETA: 0s - loss: 1.2688 - acc: 0.517 - 0s 213us/sample - loss: 1.2529 - acc: 0.5173 - val_loss: 0.9279 - val_acc: 0.5733\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.0261 - acc: 0.531 - ETA: 0s - loss: 0.9605 - acc: 0.515 - ETA: 0s - loss: 0.9813 - acc: 0.527 - ETA: 0s - loss: 1.0062 - acc: 0.535 - ETA: 0s - loss: 1.0192 - acc: 0.525 - ETA: 0s - loss: 1.0427 - acc: 0.526 - ETA: 0s - loss: 1.0105 - acc: 0.527 - ETA: 0s - loss: 0.9920 - acc: 0.535 - 0s 229us/sample - loss: 0.9876 - acc: 0.5356 - val_loss: 0.9190 - val_acc: 0.5582\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.0164 - acc: 0.593 - ETA: 0s - loss: 0.9180 - acc: 0.576 - ETA: 0s - loss: 0.9144 - acc: 0.557 - ETA: 0s - loss: 0.9141 - acc: 0.557 - ETA: 0s - loss: 0.9077 - acc: 0.551 - 1s 381us/sample - loss: 0.8930 - acc: 0.5485 - val_loss: 0.8244 - val_acc: 0.5991\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 5s - loss: 110.6509 - acc: 0.0000e+0 - ETA: 0s - loss: 46.8969 - acc: 0.4870    - ETA: 0s - loss: 33.7982 - acc: 0.49 - ETA: 0s - loss: 27.5543 - acc: 0.51 - ETA: 0s - loss: 22.9268 - acc: 0.51 - ETA: 0s - loss: 19.9884 - acc: 0.51 - 1s 276us/sample - loss: 17.5818 - acc: 0.5151 - val_loss: 3.7139 - val_acc: 0.5797\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 4.1437 - acc: 0.531 - ETA: 0s - loss: 3.3698 - acc: 0.468 - ETA: 0s - loss: 2.7820 - acc: 0.494 - ETA: 0s - loss: 2.3780 - acc: 0.483 - ETA: 0s - loss: 2.2566 - acc: 0.484 - ETA: 0s - loss: 2.0693 - acc: 0.500 - 0s 184us/sample - loss: 2.0253 - acc: 0.5000 - val_loss: 1.6469 - val_acc: 0.5711\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.7798 - acc: 0.656 - ETA: 0s - loss: 1.2961 - acc: 0.548 - ETA: 0s - loss: 1.6507 - acc: 0.538 - ETA: 0s - loss: 1.6376 - acc: 0.533 - ETA: 0s - loss: 1.5021 - acc: 0.527 - ETA: 0s - loss: 1.4140 - acc: 0.521 - 0s 165us/sample - loss: 1.4080 - acc: 0.5205 - val_loss: 1.0929 - val_acc: 0.5625\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.2169 - acc: 0.531 - ETA: 0s - loss: 0.8443 - acc: 0.554 - ETA: 0s - loss: 0.8526 - acc: 0.562 - ETA: 0s - loss: 0.8729 - acc: 0.563 - ETA: 0s - loss: 0.9401 - acc: 0.545 - 0s 146us/sample - loss: 0.9421 - acc: 0.5453 - val_loss: 1.0005 - val_acc: 0.5733\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.9536 - acc: 0.593 - ETA: 0s - loss: 0.9149 - acc: 0.483 - ETA: 0s - loss: 0.8723 - acc: 0.513 - ETA: 0s - loss: 0.8856 - acc: 0.550 - ETA: 0s - loss: 0.8627 - acc: 0.554 - ETA: 0s - loss: 0.8713 - acc: 0.552 - 0s 177us/sample - loss: 0.8722 - acc: 0.5507 - val_loss: 0.8342 - val_acc: 0.5884\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 5667fc899d820205302b38c0ec5be096</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5912356376647949</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 524</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 6s - loss: 110.3548 - acc: 0.0000e+0 - ETA: 0s - loss: 97.7313 - acc: 0.0000e+0 - ETA: 0s - loss: 76.2695 - acc: 0.0496   - ETA: 0s - loss: 59.8701 - acc: 0.11 - ETA: 0s - loss: 47.7543 - acc: 0.19 - 1s 393us/sample - loss: 45.8461 - acc: 0.2082 - val_loss: 5.4533 - val_acc: 0.4634\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 4.8971 - acc: 0.531 - ETA: 0s - loss: 6.5257 - acc: 0.464 - ETA: 0s - loss: 6.2033 - acc: 0.459 - ETA: 0s - loss: 5.6915 - acc: 0.474 - ETA: 0s - loss: 5.3110 - acc: 0.472 - ETA: 0s - loss: 4.9135 - acc: 0.477 - ETA: 0s - loss: 4.7033 - acc: 0.487 - ETA: 0s - loss: 4.6183 - acc: 0.492 - ETA: 0s - loss: 4.5010 - acc: 0.494 - 1s 428us/sample - loss: 4.4165 - acc: 0.4930 - val_loss: 2.5534 - val_acc: 0.5388\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.3400 - acc: 0.656 - ETA: 0s - loss: 2.7183 - acc: 0.571 - ETA: 0s - loss: 2.7278 - acc: 0.526 - ETA: 0s - loss: 2.7144 - acc: 0.541 - ETA: 0s - loss: 2.7073 - acc: 0.540 - ETA: 0s - loss: 2.7090 - acc: 0.526 - ETA: 0s - loss: 2.5955 - acc: 0.520 - ETA: 0s - loss: 2.6504 - acc: 0.524 - ETA: 0s - loss: 2.5886 - acc: 0.520 - 1s 309us/sample - loss: 2.6305 - acc: 0.5151 - val_loss: 1.9465 - val_acc: 0.4418\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.8643 - acc: 0.437 - ETA: 0s - loss: 1.5687 - acc: 0.477 - ETA: 0s - loss: 1.8297 - acc: 0.515 - ETA: 0s - loss: 1.7236 - acc: 0.516 - ETA: 0s - loss: 1.9145 - acc: 0.509 - ETA: 0s - loss: 1.9250 - acc: 0.524 - ETA: 0s - loss: 1.9112 - acc: 0.520 - ETA: 0s - loss: 1.9693 - acc: 0.517 - ETA: 0s - loss: 1.9798 - acc: 0.520 - 1s 272us/sample - loss: 1.9603 - acc: 0.5221 - val_loss: 1.5960 - val_acc: 0.5043\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.4500 - acc: 0.562 - ETA: 0s - loss: 1.7432 - acc: 0.540 - ETA: 0s - loss: 1.5868 - acc: 0.540 - ETA: 0s - loss: 1.6569 - acc: 0.523 - ETA: 0s - loss: 1.5917 - acc: 0.532 - ETA: 0s - loss: 1.6029 - acc: 0.535 - 0s 201us/sample - loss: 1.6371 - acc: 0.5383 - val_loss: 1.5983 - val_acc: 0.5366\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 5s - loss: 174.5513 - acc: 0.0000e+0 - ETA: 0s - loss: 144.8999 - acc: 0.0000e+0 - ETA: 0s - loss: 121.7326 - acc: 0.0000e+0 - ETA: 0s - loss: 102.8070 - acc: 0.0000e+0 - ETA: 0s - loss: 85.1895 - acc: 0.0488    - 0s 240us/sample - loss: 73.4974 - acc: 0.1165 - val_loss: 8.4201 - val_acc: 0.4634\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 14.9577 - acc: 0.46 - ETA: 0s - loss: 8.4677 - acc: 0.4396 - ETA: 0s - loss: 6.9947 - acc: 0.459 - ETA: 0s - loss: 6.2542 - acc: 0.490 - ETA: 0s - loss: 5.9821 - acc: 0.486 - 0s 147us/sample - loss: 5.8529 - acc: 0.4898 - val_loss: 3.3013 - val_acc: 0.5237\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.8402 - acc: 0.656 - ETA: 0s - loss: 5.6887 - acc: 0.474 - ETA: 0s - loss: 4.3659 - acc: 0.490 - ETA: 0s - loss: 3.8295 - acc: 0.495 - ETA: 0s - loss: 3.5352 - acc: 0.495 - ETA: 0s - loss: 3.2395 - acc: 0.504 - 0s 167us/sample - loss: 3.2342 - acc: 0.5076 - val_loss: 2.4550 - val_acc: 0.4483\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 2.8865 - acc: 0.437 - ETA: 0s - loss: 2.5447 - acc: 0.516 - ETA: 0s - loss: 2.2736 - acc: 0.521 - ETA: 0s - loss: 2.2617 - acc: 0.524 - ETA: 0s - loss: 2.2986 - acc: 0.522 - 0s 155us/sample - loss: 2.2274 - acc: 0.5264 - val_loss: 1.8721 - val_acc: 0.4892\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.4370 - acc: 0.375 - ETA: 0s - loss: 1.5965 - acc: 0.531 - ETA: 0s - loss: 1.6885 - acc: 0.510 - ETA: 0s - loss: 1.7648 - acc: 0.511 - ETA: 0s - loss: 1.7117 - acc: 0.519 - ETA: 0s - loss: 1.7540 - acc: 0.520 - 0s 177us/sample - loss: 1.7124 - acc: 0.5264 - val_loss: 1.6429 - val_acc: 0.5065\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 5s - loss: 121.8851 - acc: 0.0000e+0 - ETA: 0s - loss: 75.1351 - acc: 0.0625    - ETA: 0s - loss: 60.2674 - acc: 0.19 - ETA: 0s - loss: 47.4860 - acc: 0.26 - ETA: 0s - loss: 38.9386 - acc: 0.29 - 0s 220us/sample - loss: 37.3433 - acc: 0.3004 - val_loss: 7.8060 - val_acc: 0.4267\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 8.2422 - acc: 0.375 - ETA: 0s - loss: 6.7938 - acc: 0.500 - ETA: 0s - loss: 6.0564 - acc: 0.496 - ETA: 0s - loss: 5.7037 - acc: 0.517 - ETA: 0s - loss: 5.5266 - acc: 0.507 - 0s 160us/sample - loss: 5.2148 - acc: 0.5059 - val_loss: 3.1673 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 3.3621 - acc: 0.437 - ETA: 0s - loss: 3.6222 - acc: 0.492 - ETA: 0s - loss: 3.3885 - acc: 0.511 - ETA: 0s - loss: 3.1336 - acc: 0.501 - ETA: 0s - loss: 2.9991 - acc: 0.510 - ETA: 0s - loss: 2.9954 - acc: 0.502 - 0s 177us/sample - loss: 2.9874 - acc: 0.5005 - val_loss: 2.2354 - val_acc: 0.4569\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.4185 - acc: 0.562 - ETA: 0s - loss: 2.6677 - acc: 0.533 - ETA: 0s - loss: 2.5738 - acc: 0.520 - ETA: 0s - loss: 2.3867 - acc: 0.500 - ETA: 0s - loss: 2.3024 - acc: 0.502 - 0s 153us/sample - loss: 2.2512 - acc: 0.5081 - val_loss: 2.5363 - val_acc: 0.4483\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 2.4306 - acc: 0.531 - ETA: 0s - loss: 2.0588 - acc: 0.545 - ETA: 0s - loss: 2.0293 - acc: 0.529 - ETA: 0s - loss: 1.9622 - acc: 0.527 - ETA: 0s - loss: 1.9356 - acc: 0.504 - ETA: 0s - loss: 1.9067 - acc: 0.507 - 0s 193us/sample - loss: 1.8925 - acc: 0.5140 - val_loss: 1.6639 - val_acc: 0.4914\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: baa67d6112239f3046337d49f0261af4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5208333134651184</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 524</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 5s - loss: 123.1124 - acc: 0.0000e+0 - ETA: 0s - loss: 39.1140 - acc: 0.4353    - ETA: 0s - loss: 32.1001 - acc: 0.45 - ETA: 0s - loss: 28.3491 - acc: 0.46 - ETA: 0s - loss: 24.4918 - acc: 0.47 - 1s 422us/sample - loss: 22.4680 - acc: 0.4741 - val_loss: 10.0881 - val_acc: 0.4203\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 12.4016 - acc: 0.40 - ETA: 0s - loss: 9.4212 - acc: 0.5089 - ETA: 0s - loss: 8.2270 - acc: 0.478 - ETA: 0s - loss: 7.2493 - acc: 0.476 - ETA: 0s - loss: 6.5297 - acc: 0.467 - ETA: 0s - loss: 6.0551 - acc: 0.455 - ETA: 0s - loss: 5.5837 - acc: 0.464 - ETA: 0s - loss: 4.9774 - acc: 0.471 - ETA: 0s - loss: 4.6102 - acc: 0.479 - 1s 434us/sample - loss: 4.4769 - acc: 0.4811 - val_loss: 2.4148 - val_acc: 0.4677\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 2.0169 - acc: 0.625 - ETA: 0s - loss: 1.8117 - acc: 0.513 - ETA: 0s - loss: 1.4945 - acc: 0.495 - ETA: 0s - loss: 1.3152 - acc: 0.526 - ETA: 0s - loss: 1.3452 - acc: 0.527 - ETA: 0s - loss: 1.3254 - acc: 0.528 - ETA: 0s - loss: 1.3716 - acc: 0.529 - ETA: 0s - loss: 1.3993 - acc: 0.522 - 1s 390us/sample - loss: 1.4078 - acc: 0.5254 - val_loss: 2.0804 - val_acc: 0.5711\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.5056 - acc: 0.656 - ETA: 0s - loss: 1.5703 - acc: 0.517 - ETA: 0s - loss: 1.4617 - acc: 0.540 - ETA: 0s - loss: 1.2254 - acc: 0.568 - ETA: 0s - loss: 1.2199 - acc: 0.555 - ETA: 0s - loss: 1.1865 - acc: 0.554 - ETA: 0s - loss: 1.1385 - acc: 0.559 - ETA: 0s - loss: 1.1348 - acc: 0.552 - ETA: 0s - loss: 1.1471 - acc: 0.542 - 1s 278us/sample - loss: 1.1464 - acc: 0.5415 - val_loss: 0.9673 - val_acc: 0.4806\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.5890 - acc: 0.687 - ETA: 0s - loss: 0.8975 - acc: 0.506 - ETA: 0s - loss: 0.9959 - acc: 0.515 - ETA: 0s - loss: 0.9466 - acc: 0.531 - ETA: 0s - loss: 0.9263 - acc: 0.532 - ETA: 0s - loss: 0.9248 - acc: 0.549 - ETA: 0s - loss: 0.8978 - acc: 0.556 - ETA: 0s - loss: 0.8923 - acc: 0.556 - 0s 257us/sample - loss: 0.9083 - acc: 0.5480 - val_loss: 1.1943 - val_acc: 0.5647\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 6s - loss: 128.6765 - acc: 0.0000e+0 - ETA: 0s - loss: 33.8607 - acc: 0.4231    - ETA: 0s - loss: 29.5360 - acc: 0.41 - ETA: 0s - loss: 26.5193 - acc: 0.43 - ETA: 0s - loss: 23.9610 - acc: 0.45 - 0s 239us/sample - loss: 22.0383 - acc: 0.4649 - val_loss: 7.9579 - val_acc: 0.4116\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 16.7642 - acc: 0.43 - ETA: 0s - loss: 5.5615 - acc: 0.5000 - ETA: 0s - loss: 4.9507 - acc: 0.500 - ETA: 0s - loss: 4.6809 - acc: 0.492 - ETA: 0s - loss: 5.0517 - acc: 0.502 - 0s 166us/sample - loss: 4.8798 - acc: 0.5022 - val_loss: 1.7999 - val_acc: 0.5065\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.5498 - acc: 0.437 - ETA: 0s - loss: 2.0177 - acc: 0.507 - ETA: 0s - loss: 2.0484 - acc: 0.523 - ETA: 0s - loss: 1.7892 - acc: 0.523 - ETA: 0s - loss: 1.7187 - acc: 0.520 - ETA: 0s - loss: 1.5799 - acc: 0.520 - ETA: 0s - loss: 1.5199 - acc: 0.524 - ETA: 0s - loss: 1.4593 - acc: 0.536 - 0s 249us/sample - loss: 1.4414 - acc: 0.5351 - val_loss: 1.2327 - val_acc: 0.5366\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.9387 - acc: 0.625 - ETA: 0s - loss: 1.1401 - acc: 0.514 - ETA: 0s - loss: 1.0306 - acc: 0.515 - ETA: 0s - loss: 1.0491 - acc: 0.537 - ETA: 0s - loss: 1.0872 - acc: 0.552 - ETA: 0s - loss: 1.0789 - acc: 0.546 - ETA: 0s - loss: 1.1402 - acc: 0.535 - 0s 214us/sample - loss: 1.1602 - acc: 0.5275 - val_loss: 1.2507 - val_acc: 0.4634\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.0167 - acc: 0.531 - ETA: 0s - loss: 1.1727 - acc: 0.531 - ETA: 0s - loss: 1.2060 - acc: 0.522 - ETA: 0s - loss: 1.1131 - acc: 0.520 - ETA: 0s - loss: 1.1093 - acc: 0.517 - 1s 375us/sample - loss: 1.0640 - acc: 0.5200 - val_loss: 0.9929 - val_acc: 0.5733\n",
      "Train on 1854 samples, validate on 464 samples\n",
      "Epoch 1/5\n",
      "1854/1854 [==============================] - ETA: 5s - loss: 106.4270 - acc: 0.0000e+0 - ETA: 0s - loss: 41.5639 - acc: 0.4519    - ETA: 0s - loss: 32.5646 - acc: 0.46 - ETA: 0s - loss: 26.5064 - acc: 0.47 - ETA: 0s - loss: 21.9757 - acc: 0.48 - 0s 231us/sample - loss: 19.8858 - acc: 0.4914 - val_loss: 4.1950 - val_acc: 0.4504\n",
      "Epoch 2/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 10.6430 - acc: 0.43 - ETA: 0s - loss: 3.8325 - acc: 0.4933 - ETA: 0s - loss: 2.7315 - acc: 0.514 - ETA: 0s - loss: 2.4336 - acc: 0.513 - ETA: 0s - loss: 2.2721 - acc: 0.505 - 0s 176us/sample - loss: 2.1247 - acc: 0.5129 - val_loss: 1.2838 - val_acc: 0.5582\n",
      "Epoch 3/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.4758 - acc: 0.468 - ETA: 0s - loss: 1.3057 - acc: 0.531 - ETA: 0s - loss: 1.2627 - acc: 0.568 - ETA: 0s - loss: 1.3994 - acc: 0.528 - ETA: 0s - loss: 1.5253 - acc: 0.535 - ETA: 0s - loss: 1.5947 - acc: 0.534 - ETA: 0s - loss: 1.9101 - acc: 0.537 - 0s 209us/sample - loss: 1.8523 - acc: 0.5297 - val_loss: 1.1006 - val_acc: 0.5259\n",
      "Epoch 4/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 0.6813 - acc: 0.750 - ETA: 0s - loss: 1.0096 - acc: 0.575 - ETA: 0s - loss: 1.0942 - acc: 0.551 - ETA: 0s - loss: 1.0453 - acc: 0.546 - ETA: 0s - loss: 1.0178 - acc: 0.538 - 0s 159us/sample - loss: 1.0125 - acc: 0.5405 - val_loss: 1.1604 - val_acc: 0.3966\n",
      "Epoch 5/5\n",
      "1854/1854 [==============================] - ETA: 0s - loss: 1.2061 - acc: 0.437 - ETA: 0s - loss: 0.9215 - acc: 0.536 - ETA: 0s - loss: 0.9230 - acc: 0.547 - ETA: 0s - loss: 0.9203 - acc: 0.551 - ETA: 0s - loss: 0.9240 - acc: 0.546 - ETA: 0s - loss: 0.9953 - acc: 0.540 - 0s 183us/sample - loss: 1.0078 - acc: 0.5394 - val_loss: 1.3589 - val_acc: 0.4181\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 4a049d5150f2b749cf15f331f14cdfc5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5675287246704102</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 492</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, Y_train,\n",
    "             epochs=5,\n",
    "             validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Results summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Results in my_dir3\\helloworld3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Showing 10 best trials</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective(name='val_acc', direction='max')</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 3c077943e8be1a7577b04218f843b1fd</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5711206793785095</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 428</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f9e875ed2c938b0222da0e66ac3a9cf5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5675287246704102</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 364</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 291f160fd80e07f7c21b0191e8d85ef5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5625</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 524</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 9d944d24e13430533fafbd628f926f01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5517241358757019</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 492</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 8f8a567e679bcc594b5b608ed4bfe9b6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5445402264595032</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout in the input layer with weight constraint\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "Y_train = encoder.transform(Y_train)\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(41,)))\n",
    "    model.add(Dense(60, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "    sgd = SGD(lr=0.1, momentum=0.9)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1(neurons=1):\n",
    "\t# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=41, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.007012 using {'activation': 'softmax'}\n",
      "0.007012 (0.002018) with: {'activation': 'softmax'}\n",
      "0.007012 (0.002018) with: {'activation': 'softplus'}\n",
      "0.007012 (0.002018) with: {'activation': 'softsign'}\n",
      "0.007012 (0.002018) with: {'activation': 'relu'}\n",
      "0.007012 (0.002018) with: {'activation': 'tanh'}\n",
      "0.007012 (0.002018) with: {'activation': 'sigmoid'}\n",
      "0.007012 (0.002018) with: {'activation': 'hard_sigmoid'}\n",
      "0.007012 (0.002018) with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the activation function\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu'):\n",
    "\t# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=41, kernel_initializer='uniform', activation=activation))\n",
    "    model.add(Dense(24, input_dim=41, kernel_initializer='uniform', activation=activation))\n",
    "    model.add(Dense(24, input_dim=41, kernel_initializer='uniform', activation=activation))\n",
    "    model.add(Dense(12, input_dim=41, kernel_initializer='uniform', activation=activation))\n",
    "    model.add(Dense(8, input_dim=41, kernel_initializer='uniform', activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.007012 using {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.8, 'weight_constraint': 5}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.9, 'weight_constraint': 1}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.9, 'weight_constraint': 2}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.9, 'weight_constraint': 3}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.9, 'weight_constraint': 4}\n",
      "0.007012 (0.002018) with: {'dropout_rate': 0.9, 'weight_constraint': 5}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the dropout rate\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(dropout_rate=0.0, weight_constraint=0):\n",
    "\t# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=41, kernel_initializer='uniform', activation='softmax', kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dense(24, activation='softmax'))\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.007012 using {'neurons': 1}\n",
      "0.007012 (0.002018) with: {'neurons': 1}\n",
      "0.007012 (0.002018) with: {'neurons': 5}\n",
      "0.007012 (0.002018) with: {'neurons': 10}\n",
      "0.007012 (0.002018) with: {'neurons': 15}\n",
      "0.007012 (0.002018) with: {'neurons': 20}\n",
      "0.007012 (0.002018) with: {'neurons': 25}\n",
      "0.007012 (0.002018) with: {'neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the number of neurons\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(neurons=1):\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=41, kernel_initializer='uniform', activation='softmax', kernel_constraint=maxnorm(4)))\n",
    "    model.add(Dense(24, activation='softmax'))\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.add(Dropout(0))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
